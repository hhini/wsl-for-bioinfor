{
  "hash": "048baa565b46e5c3d138a58b4ea037b9",
  "result": {
    "engine": "jupyter",
    "markdown": "first to load the data\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sqlalchemy import create_engine\nimport duckdb\nimport io\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclinical_outfile = r\"E:\\acode\\wsl\\data2\\GDCdata\\TCGA-LUSC\\clinical_metadata.csv\"\nclinical_df = pd.read_csv(clinical_outfile)\n# Load RNA expression matrix\nexpression_file = \"E:/acode/wsl/data2/GDCdata/TCGA-LUSC/rnaexp.csv\"\nexpression_df = pd.read_csv(expression_file, index_col=0)\nprint(\"‚úÖ Expression matrix loaded. Shape:\", expression_df.shape)\n\n# Optional: Preview\nprint(expression_df.iloc[:5, :5])\nprint(clinical_df.iloc[:5, :5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ Expression matrix loaded. Shape: (60664, 563)\n  gene_name  TCGA-90-A4EE  TCGA-85-8049  TCGA-33-6737  TCGA-39-5016\n1    TSPAN6        8.4836        8.9006       24.1585       21.0036\n2      TNMD        0.0149        0.0000        0.0000        0.0587\n3      DPM1       35.1344       25.3093       23.1613       63.1033\n4     SCYL3        3.9120        2.5760        1.2931        3.3603\n5  C1orf112        3.7604        2.8456        2.6190        2.7706\n        case_id vital_status  days_to_death  days_to_last_followup  age_at_dx\n0  TCGA-85-6561        Alive            NaN                   24.0       66.0\n1  TCGA-37-5819        Alive            NaN                  103.0       64.0\n2  TCGA-77-8008         Dead         2639.0                    NaN       68.0\n3  TCGA-56-8083        Alive            NaN                    2.0       56.0\n4  TCGA-56-A4ZJ        Alive            NaN                   25.0       75.0\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclinical_ids = set(clinical_df['case_id'])\nexpression_ids = set(expression_df.columns)\noverlap = clinical_ids.intersection(expression_ids)\nprint(f\"üß™ Overlapping samples: {len(overlap)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüß™ Overlapping samples: 501\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(clinical_df.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex(['case_id', 'vital_status', 'days_to_death', 'days_to_last_followup',\n       'age_at_dx', 'stage', 'gender', 'tumor_status', 'smoking_history',\n       'pack_years_smoked', 'residual_tumor', 'ecog_score',\n       'radiation_therapy', 'primary_therapy_outcome'],\n      dtype='object')\n```\n:::\n:::\n\n\n# ML with clinical data and genes to predict\n\n## üß¨ Step 1: Define Your Gene List\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ngene_list = [\n    'HSD17B14', 'CLTA', 'C9orf72', 'TTC39A', 'C4orf48', 'TESK1', 'FAM241B',\n    'C5', 'TP53I3', 'SLC39A4', 'CLDN3', 'NRIP3', 'PYCR1', 'MT1M', 'H19',\n    'PSD3', 'ETNK1', 'HMGB3', 'TRPM2', 'MELK', 'HILPDA', 'ATP23', 'IFFO2',\n    'TMEM80', 'PRAG1', 'SVIP'\n]\ngene_list_sql = ', '.join([f\"'{gene}'\" for gene in gene_list])\n```\n:::\n\n\n## üß™ Step 2: Unpivot RNA Expression Matrix in DuckDB\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# --- 2. DuckDB Setup ---\n\n# Create an in-memory database or a file-backed one ('path/to/my.duckdb')\ncon = duckdb.connect(database=':memory:', read_only=False)\nprint(\"‚úÖ DuckDB connection established (in-memory).\")\n\n# --- Loading Data into DuckDB ---\n# DuckDB can read directly from pandas DataFrames using the 'register' function.\n\n# Register the DataFrames as temporary tables in DuckDB\ncon.register('clinical_metadata_db', clinical_df)\ncon.register('rna_expression_matrix_db', expression_df)\n\nprint(\"‚úÖ DataFrames registered as tables in DuckDB.\")\n\n# --- Verification Query in DuckDB ---\n# Example: Querying the registered tables\nduckdb_query = \"\"\"\nSELECT t1.case_id, t1.vital_status, t2.\"TSPAN6\", t2.\"DPM1\"\nFROM clinical_metadata_db t1\nINNER JOIN rna_expression_matrix_db t2\n    ON t1.case_id = t2.case_id  -- Note: we'll need to ensure case_id is a column in expression_df for a direct join!\nLIMIT 5;\n\"\"\"\n# Based on wer data structure, we might need to UNPIVOT the RNA data first in DuckDB\n# The join key is `case_id` from clinical_df and the column *names* (TCGA-...) in expression_df.\n\nprint(\"\\n--- DuckDB Verification Query (Sample) ---\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ DuckDB connection established (in-memory).\n‚úÖ DataFrames registered as tables in DuckDB.\n\n--- DuckDB Verification Query (Sample) ---\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nexpression_cols = [col for col in expression_df.columns if col.startswith('TCGA-')]\n#This grabs all sample columns (e.g. TCGA-18-3406, TCGA-18-3407) from your expression matrix.\nunpivot_query = f\"\"\"\nCREATE TEMP TABLE rna_expression_unpivoted AS\nSELECT\n    gene_name,\n    s.case_id AS sample_id,\n    s.expression AS expression_value\nFROM (\n    SELECT\n        gene_name,\n        unnest(list_value(\n            {', '.join([\n                f\"struct_pack(case_id := '{col}', expression := \\\"{col}\\\")\"\n                for col in expression_cols\n            ])}\n        )) AS s\n    FROM rna_expression_matrix_db\n);\n\"\"\"\ncon.execute(unpivot_query)\n```\n\n::: {.cell-output .cell-output-display}\n```\nFloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<_duckdb.DuckDBPyConnection at 0x23a427fc4f0>\n```\n:::\n:::\n\n\nThis part:\n\n``` python\nf\"struct_pack(case_id := '{col}', expression := \\\"{col}\\\")\" \n```\n\nis inside a Python f-string. Here's what each part means:\n\n-   `'case_id := '{col}'` ‚Üí sets the sample ID as a string (e.g. `'TCGA-18-3406'`)\n\n-   `expression := \\\"{col}\\\"` ‚Üí references the actual column in DuckDB\n\nYou're building a **Python string** that will be embedded inside a **SQL string**. The backslash escapes the double quotes so Python doesn‚Äôt treat them as the end of the string\n\n### What This Does:\n\n-   `struct_pack(...)` creates a mini-record for each sample: `{case_id: 'TCGA-...', expression: value}`\n\n-   `list_value(...)` builds a list of these structs for each gene\n\n-   `unnest(...)` explodes that list into rows\n\n-   Final result: one row per `(gene_name, sample_id, expression_value)`\n\n## üîó Step 3: Join with Clinical Metadata\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsurvival_query = f\"\"\"\nSELECT\n    t1.case_id,\n    t1.vital_status,\n    COALESCE(t1.days_to_death, t1.days_to_last_followup) AS survival_days,\n    t1.age_at_dx,\n    t1.stage,\n    t1.gender,\n    t1.tumor_status,\n    t1.smoking_history,\n    t1.pack_years_smoked,\n    t2.gene_name,\n    t2.expression_value\nFROM clinical_metadata_db t1\nINNER JOIN rna_expression_unpivoted t2\n    ON t1.case_id = t2.sample_id\nWHERE t2.gene_name IN ({gene_list_sql})\n\"\"\"\nsurvival_df_long = con.execute(survival_query).fetchdf()\n```\n:::\n\n\n## üìä Step 4: Pivot to Wide Format\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nsurvival_df = survival_df_long.pivot_table(\n    index=[\n        'case_id', 'vital_status', 'survival_days', 'age_at_dx',\n        'stage', 'gender', 'tumor_status', 'smoking_history', 'pack_years_smoked'\n    ],\n    columns='gene_name',\n    values='expression_value'\n).reset_index()\n```\n:::\n\n\n## üíæ Step 5: Save to CSV\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nsurvival_df.to_csv(r\"E:\\acode\\wsl\\data2\\GDCdata\\TCGA-LUSC\\survival_data_27genes.csv\", index=False)\nprint(\"‚úÖ Survival-ready dataset with 27 genes and full clinical metadata saved.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ Survival-ready dataset with 27 genes and full clinical metadata saved.\n```\n:::\n:::\n\n\n### üîç Root Cause: Gene Filtering + Expression Missingness\n\nEven though you're preserving clinical samples and deferring imputation, your query still includes this line:\n\n``` sql\nWHERE t2.gene_name IN ({gene_list_sql}) \n```\n\nThis filters the RNA expression to only your **27 selected genes**. So if a sample is **missing expression for even one of those genes**, it won‚Äôt appear in the long-format result. And since you're pivoting afterward, only samples with **at least one matching expression value** survive.\n\n### üß† What‚Äôs Actually Happening\n\n-   You have \\~500 clinical samples.\n\n-   But only 311 have **expression values for at least one of your 27 genes**.\n\n-   The rest are dropped during the `INNER JOIN` or because they don‚Äôt match the `WHERE` clause.\n\n### If we want to get the overlapping data\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# --- 2. DuckDB Setup ---\n\n# Create an in-memory database or a file-backed one ('path/to/my.duckdb')\ncon = duckdb.connect(database=':memory:', read_only=False)\nprint(\"‚úÖ DuckDB connection established (in-memory).\")\n\n# --- Loading Data into DuckDB ---\n# DuckDB can read directly from pandas DataFrames using the 'register' function.\n\n# Register the DataFrames as temporary tables in DuckDB\ncon.register('clinical_metadata_db', clinical_df)\ncon.register('rna_expression_matrix_db', expression_df)\n\nprint(\"‚úÖ DataFrames registered as tables in DuckDB.\")\n\n# --- Verification Query in DuckDB ---\n# Example: Querying the registered tables\nduckdb_query = \"\"\"\nSELECT t1.case_id, t1.vital_status, t2.\"TSPAN6\", t2.\"DPM1\"\nFROM clinical_metadata_db t1\nINNER JOIN rna_expression_matrix_db t2\n    ON t1.case_id = t2.case_id  -- Note: we'll need to ensure case_id is a column in expression_df for a direct join!\nLIMIT 5;\n\"\"\"\n# Based on wer data structure, we might need to UNPIVOT the RNA data first in DuckDB\n# The join key is `case_id` from clinical_df and the column *names* (TCGA-...) in expression_df.\n\nprint(\"\\n--- DuckDB Verification Query (Sample) ---\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ DuckDB connection established (in-memory).\n‚úÖ DataFrames registered as tables in DuckDB.\n\n--- DuckDB Verification Query (Sample) ---\n```\n:::\n:::\n\n\n### ‚úÖ Step 1: Unpivot RNA Expression Matrix (same as before)\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nexpression_cols = [col for col in expression_df.columns if col.startswith('TCGA-')]\n\nunpivot_query = f\"\"\"\nCREATE TEMP TABLE rna_expression_unpivoted AS\nSELECT\n    gene_name,\n    s.case_id AS sample_id,\n    s.expression AS expression_value\nFROM (\n    SELECT\n        gene_name,\n        unnest(list_value(\n            {', '.join([\n                f\"struct_pack(case_id := '{col}', expression := \\\"{col}\\\")\"\n                for col in expression_cols\n            ])}\n        )) AS s\n    FROM rna_expression_matrix_db\n);\n\"\"\"\ncon.execute(unpivot_query)\n```\n\n::: {.cell-output .cell-output-display}\n```\nFloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n<_duckdb.DuckDBPyConnection at 0x23a42be3bf0>\n```\n:::\n:::\n\n\n### üîó Step 2: LEFT JOIN with Clinical Metadata\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ngene_list_sql = ', '.join([f\"'{gene}'\" for gene in gene_list])\n\nsurvival_query = f\"\"\"\nSELECT\n    t1.case_id,\n    t1.vital_status,\n    t1.age_at_dx,\n    t1.stage,\n    t1.gender,\n    t1.tumor_status,\n    t1.smoking_history,\n    t1.pack_years_smoked,\n    t2.gene_name,\n    t2.expression_value\nFROM clinical_metadata_db t1\nLEFT JOIN rna_expression_unpivoted t2\n    ON t1.case_id = t2.sample_id\nWHERE t2.gene_name IN ({gene_list_sql})\n\"\"\"\nsurvival_df_long = con.execute(survival_query).fetchdf()\n```\n:::\n\n\n### üìä Pivot with Minimal Index, Add Clinical Later\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\npivot_df = survival_df_long.pivot_table(\n    index=['case_id'],\n    columns='gene_name',\n    values='expression_value'\n).reset_index()\n\n# Merge back clinical metadata\nclinical_cols = ['case_id', 'vital_status', 'age_at_dx',\n                 'stage', 'gender', 'tumor_status', 'smoking_history', 'pack_years_smoked']\nclinical_subset = survival_df_long[clinical_cols].drop_duplicates(subset='case_id')\n\nsurvival_df = pivot_df.merge(clinical_subset, on='case_id', how='left')\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nsurvival_df.to_csv(r\"E:\\acode\\wsl\\data2\\GDCdata\\TCGA-LUSC\\survival_data_27genes2.csv\", index=False)\nprint(\"‚úÖ Survival-ready dataset with 27 genes and full clinical metadata saved.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ Survival-ready dataset with 27 genes and full clinical metadata saved.\n```\n:::\n:::\n\n\n## üßº Step 1: Preprocess Stage and Event\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.read_csv(r\"E:\\acode\\wsl\\data2\\GDCdata\\TCGA-LUSC\\survival_data_27genes.csv\")\n\n# Simplify stage\ndef simplify_stage(stage):\n    if pd.isna(stage):\n        return \"Unknown\"\n    elif stage in [\"Stage I\", \"Stage IA\", \"Stage IB\", \"Stage II\", \"Stage IIA\", \"Stage IIB\"]:\n        return \"Early/Local (Stage I-II)\"\n    elif stage in [\"Stage III\", \"Stage IIIA\", \"Stage IIIB\", \"Stage IV\", \"Stage IVA\", \"Stage IVB\"]:\n        return \"Advanced (Stage III IV)\"\n    else:\n        return \"Other/Unknown\"\n\ndf[\"simplified_stage\"] = df[\"stage\"].apply(simplify_stage)\n\n# Create binary event\ndf[\"event\"] = df[\"vital_status\"].map({\"Dead\": 1, \"Alive\": 0})\n```\n:::\n\n\ncheck the data\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nprint(df.head(5))\nprint(df.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        case_id vital_status  survival_days  age_at_dx       stage  gender  \\\n0  TCGA-18-3406         Dead          371.0       67.0    Stage IA    MALE   \n1  TCGA-18-3407         Dead          136.0       72.0    Stage IB    MALE   \n2  TCGA-18-3408        Alive         2099.0       77.0    Stage IB  FEMALE   \n3  TCGA-18-3409        Alive         2417.0       74.0    Stage IA    MALE   \n4  TCGA-18-3411        Alive         1415.0       63.0  Stage IIIA  FEMALE   \n\n  tumor_status  smoking_history  pack_years_smoked   ATP23  ...    PYCR1  \\\n0   WITH TUMOR              4.0              200.0  5.6033  ...  11.8991   \n1   TUMOR FREE              3.0               40.0  0.8756  ...  13.3749   \n2   WITH TUMOR              4.0               30.0  4.7196  ...   2.7003   \n3   TUMOR FREE              3.0               20.0  2.5146  ...  11.6042   \n4   TUMOR FREE              2.0               50.0  4.6513  ...  44.2339   \n\n   SLC39A4    SVIP    TESK1  TMEM80   TP53I3   TRPM2  TTC39A  \\\n0   2.0324  7.0555  11.0774  1.9570   6.3318  2.2126  1.7314   \n1   3.3303  1.1290  13.5017  5.4117   6.1919  1.8058  0.7852   \n2   0.3969  2.1804   7.7895  0.8571   6.1753  0.3600  1.2734   \n3   7.3355  5.9671   8.2695  3.9009   5.4604  1.4607  0.3505   \n4  11.1194  0.5988   9.0761  2.0017  11.4973  3.9703  1.8747   \n\n           simplified_stage  event  \n0  Early/Local (Stage I-II)      1  \n1  Early/Local (Stage I-II)      1  \n2  Early/Local (Stage I-II)      0  \n3  Early/Local (Stage I-II)      0  \n4   Advanced (Stage III IV)      0  \n\n[5 rows x 37 columns]\nIndex(['case_id', 'vital_status', 'survival_days', 'age_at_dx', 'stage',\n       'gender', 'tumor_status', 'smoking_history', 'pack_years_smoked',\n       'ATP23', 'C4orf48', 'C5', 'C9orf72', 'CLDN3', 'CLTA', 'ETNK1',\n       'FAM241B', 'H19', 'HILPDA', 'HMGB3', 'HSD17B14', 'IFFO2', 'MELK',\n       'MT1M', 'NRIP3', 'PRAG1', 'PSD3', 'PYCR1', 'SLC39A4', 'SVIP', 'TESK1',\n       'TMEM80', 'TP53I3', 'TRPM2', 'TTC39A', 'simplified_stage', 'event'],\n      dtype='object')\n```\n:::\n:::\n\n\n``` python\n>>> print(df.head(5))\n... print(df.columns)\n        case_id vital_status  ...          simplified_stage  event\n0  TCGA-18-3406         Dead  ...  Early/Local (Stage I-II)      1\n1  TCGA-18-3407         Dead  ...  Early/Local (Stage I-II)      1\n2  TCGA-18-3408        Alive  ...  Early/Local (Stage I-II)      0\n3  TCGA-18-3409        Alive  ...  Early/Local (Stage I-II)      0\n4  TCGA-18-3411        Alive  ...   Advanced (Stage III IV)      0\n\n[5 rows x 37 columns]\nIndex(['case_id', 'vital_status', 'survival_days', 'age_at_dx', 'stage',\n       'gender', 'tumor_status', 'smoking_history', 'pack_years_smoked', 'ATP23',\n       'C4orf48', 'C5', 'C9orf72', 'CLDN3', 'CLTA', 'ETNK1', 'FAM241B', 'H19',\n       'HILPDA', 'HMGB3', 'HSD17B14', 'IFFO2', 'MELK', 'MT1M', 'NRIP3', 'PRAG1',\n       'PSD3', 'PYCR1', 'SLC39A4', 'SVIP', 'TESK1', 'TMEM80', 'TP53I3', 'TRPM2',\n       'TTC39A', 'simplified_stage', 'event'],\n      dtype='object')\n```\n\n### üßπ Step 1: Drop Unnecessary Columns\n\nSince `vital_status` and `stage` are now encoded into `event` and `simplified_stage`, you can drop them:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ndf = df.drop(columns=[\"vital_status\", \"stage\"])\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndf = df.drop(columns=[\"survival_days\"])\n```\n:::\n\n\n### üîç Step 2: Check Unique Values for Categorical Features\n\nThis helps you understand how to encode them:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nprint(\"Gender:\", df[\"gender\"].unique())\nprint(\"Tumor Status:\", df[\"tumor_status\"].unique())\nprint(\"Simplified Stage:\", df[\"simplified_stage\"].unique())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGender: ['MALE' 'FEMALE']\nTumor Status: ['WITH TUMOR' 'TUMOR FREE']\nSimplified Stage: ['Early/Local (Stage I-II)' 'Advanced (Stage III IV)']\n```\n:::\n:::\n\n\n``` python\nGender: ['MALE' 'FEMALE']\nTumor Status: ['WITH TUMOR' 'TUMOR FREE']\nSimplified Stage: ['Early/Local (Stage I-II)' 'Advanced (Stage III IV)']\n```\n\n### üîÑ Step 3: Encode Categorical Variables\n\nYou can use label encoding or one-hot encoding. For Discriminant Analysis, label encoding is often sufficient:\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import LabelEncoder\n\nfor col in [\"gender\", \"tumor_status\", \"simplified_stage\"]:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col])\n```\n:::\n\n\nThis will convert:\n\n-   `gender`: MALE ‚Üí 1, FEMALE ‚Üí 0\n\n-   `tumor_status`: WITH TUMOR ‚Üí 1, TUMOR FREE ‚Üí 0\n\n-   `simplified_stage`: Encoded numerically (e.g., Early/Local ‚Üí 0, Advanced ‚Üí 1, etc.)\n\n### üß™ Step 4: Prepare Features and Target\n\nNow you're ready to split features and target:\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nX = df.drop(columns=[\"case_id\", \"event\"])  \n# Drop ID and target \ny = df[\"event\"]\n```\n:::\n\n\ncheck the data right now\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nprint(X.head(5))\nprint(X.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   age_at_dx  gender  tumor_status  smoking_history  pack_years_smoked  \\\n0       67.0       1             1              4.0              200.0   \n1       72.0       1             0              3.0               40.0   \n2       77.0       0             1              4.0               30.0   \n3       74.0       1             0              3.0               20.0   \n4       63.0       0             0              2.0               50.0   \n\n    ATP23  C4orf48      C5  C9orf72   CLDN3  ...     PSD3    PYCR1  SLC39A4  \\\n0  5.6033   2.8345  0.9613   3.4351  0.8934  ...   2.0129  11.8991   2.0324   \n1  0.8756   1.3487  2.4564   7.9812  0.7860  ...   6.6344  13.3749   3.3303   \n2  4.7196   0.0538  2.4395  15.6769  0.0228  ...   9.5953   2.7003   0.3969   \n3  2.5146   1.2988  0.9280   1.5068  1.6712  ...  16.6347  11.6042   7.3355   \n4  4.6513   2.5755  0.8727   3.3174  2.7984  ...   3.6455  44.2339  11.1194   \n\n     SVIP    TESK1  TMEM80   TP53I3   TRPM2  TTC39A  simplified_stage  \n0  7.0555  11.0774  1.9570   6.3318  2.2126  1.7314                 1  \n1  1.1290  13.5017  5.4117   6.1919  1.8058  0.7852                 1  \n2  2.1804   7.7895  0.8571   6.1753  0.3600  1.2734                 1  \n3  5.9671   8.2695  3.9009   5.4604  1.4607  0.3505                 1  \n4  0.5988   9.0761  2.0017  11.4973  3.9703  1.8747                 0  \n\n[5 rows x 32 columns]\nIndex(['age_at_dx', 'gender', 'tumor_status', 'smoking_history',\n       'pack_years_smoked', 'ATP23', 'C4orf48', 'C5', 'C9orf72', 'CLDN3',\n       'CLTA', 'ETNK1', 'FAM241B', 'H19', 'HILPDA', 'HMGB3', 'HSD17B14',\n       'IFFO2', 'MELK', 'MT1M', 'NRIP3', 'PRAG1', 'PSD3', 'PYCR1', 'SLC39A4',\n       'SVIP', 'TESK1', 'TMEM80', 'TP53I3', 'TRPM2', 'TTC39A',\n       'simplified_stage'],\n      dtype='object')\n```\n:::\n:::\n\n\n## pick the important features\n\n## üå≤ Option 1: Random Forest Feature Importance\n\nRandom Forests are great for estimating feature relevance, even if you're ultimately using Discriminant Analysis. Here's how to extract importance scores:\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Train a Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# Get feature importances\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nimportances_sorted = importances.sort_values(ascending=False)\n\n# Plot top features\nplt.figure(figsize=(10, 6))\nimportances_sorted.head(20).plot(kind='barh')\nplt.title(\"Top 20 Feature Importances (Random Forest)\")\nplt.xlabel(\"Importance Score\")\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mlsql_files/figure-docx/cell-25-output-1.png){}\n:::\n:::\n\n\n![](plot-30.png)\n\ncheck\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nprint(importances_sorted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntumor_status         0.125528\nH19                  0.040419\npack_years_smoked    0.039808\nage_at_dx            0.039261\nC4orf48              0.037028\nCLDN3                0.035837\nSLC39A4              0.034131\nC5                   0.033265\nATP23                0.033165\nPRAG1                0.032984\nTMEM80               0.032625\nMELK                 0.032337\nSVIP                 0.032040\nC9orf72              0.031263\nTESK1                0.030438\nHSD17B14             0.030422\nFAM241B              0.028915\nETNK1                0.028655\nTTC39A               0.028447\nPYCR1                0.028165\nPSD3                 0.027984\nNRIP3                0.027635\nCLTA                 0.026768\nTP53I3               0.026439\nHMGB3                0.026315\nTRPM2                0.026061\nHILPDA               0.024921\nMT1M                 0.023093\nIFFO2                0.021146\nsmoking_history      0.006051\nsimplified_stage     0.004483\ngender               0.004372\ndtype: float64\n```\n:::\n:::\n\n\n``` python\n>>> print(importances_sorted)\ntumor_status         0.125528\nH19                  0.040419\npack_years_smoked    0.039808\nage_at_dx            0.039261\nC4orf48              0.037028\nCLDN3                0.035837\nSLC39A4              0.034131\nC5                   0.033265\nATP23                0.033165\nPRAG1                0.032984\nTMEM80               0.032625\nMELK                 0.032337\nSVIP                 0.032040\nC9orf72              0.031263\nTESK1                0.030438\nHSD17B14             0.030422\nFAM241B              0.028915\nETNK1                0.028655\nTTC39A               0.028447\nPYCR1                0.028165\nPSD3                 0.027984\nNRIP3                0.027635\nCLTA                 0.026768\nTP53I3               0.026439\nHMGB3                0.026315\nTRPM2                0.026061\nHILPDA               0.024921\nMT1M                 0.023093\nIFFO2                0.021146\nsmoking_history      0.006051\nsimplified_stage     0.004483\ngender               0.004372\ndtype: float64\n```\n\n## üìä Option 2: Correlation with Target (`event`)\n\nThis gives a quick statistical view of how each feature relates to the binary outcome:\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ncorrelations = df.drop(columns=[\"case_id\"]).corr()[\"event\"].abs().sort_values(ascending=False)\nprint(correlations.head(20))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nevent               1.000000\ntumor_status        0.505101\nage_at_dx           0.141032\nH19                 0.124147\nsimplified_stage    0.115560\ngender              0.088613\nHSD17B14            0.080875\nCLDN3               0.076875\nTRPM2               0.072143\nPYCR1               0.070471\nTMEM80              0.070332\nIFFO2               0.060668\nFAM241B             0.054786\nSVIP                0.054521\nMELK                0.051436\nNRIP3               0.051194\nHMGB3               0.050364\nETNK1               0.045469\nTP53I3              0.042114\nC5                  0.039073\nName: event, dtype: float64\n```\n:::\n:::\n\n\n``` python\n... print(correlations.head(20))\nevent               1.000000\ntumor_status        0.505101\nage_at_dx           0.141032\nH19                 0.124147\nsimplified_stage    0.115560\ngender              0.088613\nHSD17B14            0.080875\nCLDN3               0.076875\nTRPM2               0.072143\nPYCR1               0.070471\nTMEM80              0.070332\nIFFO2               0.060668\nFAM241B             0.054786\nSVIP                0.054521\nMELK                0.051436\nNRIP3               0.051194\nHMGB3               0.050364\nETNK1               0.045469\nTP53I3              0.042114\nC5                  0.039073\nName: event, dtype: float64\n```\n\nThis helps you spot features with strong linear relationships to the target.\n\n## üß™ Step 2: LASSO for Gene Selection\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.preprocessing import StandardScaler\ngene_list = [\n    'HSD17B14', 'CLTA', 'C9orf72', 'TTC39A', 'C4orf48', 'TESK1', 'FAM241B',\n    'C5', 'TP53I3', 'SLC39A4', 'CLDN3', 'NRIP3', 'PYCR1', 'MT1M', 'H19',\n    'PSD3', 'ETNK1', 'HMGB3', 'TRPM2', 'MELK', 'HILPDA', 'ATP23', 'IFFO2',\n    'TMEM80', 'PRAG1', 'SVIP'\n]\n\ngene_cols = [col for col in df.columns if col in gene_list]  # your 27 genes\nX = df[gene_cols]\ny = df[\"event\"]\n\n# Normalize gene expression\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# LASSO with cross-validation\nlasso = LassoCV(cv=5, random_state=42)\nlasso.fit(X_scaled, y)\n\n# Extract selected genes\nselected_genes = [gene for gene, coef in zip(gene_cols, lasso.coef_) if abs(coef) > 1e-4]\nprint(\"üß¨ Selected genes via LASSO:\", selected_genes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüß¨ Selected genes via LASSO: []\n```\n:::\n:::\n\n\n## üìä Step 3: Check Relevance to Stage\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor gene in selected_genes:\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(x=\"simplified_stage\", y=gene, data=df)\n    plt.title(f\"{gene} Expression vs. Stage\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nfrom sklearn.linear_model import ElasticNetCV\n\nenet = ElasticNetCV(cv=5, l1_ratio=0.7, random_state=42)\nenet.fit(X_scaled, y)\nselected_genes = [gene for gene, coef in zip(gene_cols, enet.coef_) if abs(coef) > 1e-4]\nprint(\"üß¨ Selected genes via ElasticNet:\", selected_genes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüß¨ Selected genes via ElasticNet: []\n```\n:::\n:::\n\n\n### ‚úÖ Step-by-Step Code to Select Overlapping Important Features\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 1. Train Random Forest and get importances\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\nrf_importances = pd.Series(rf.feature_importances_, index=X.columns)\n\n# 2. Compute correlation with target\ncorrelations = df[X.columns.tolist() + ['event']].corr()['event'].drop('event').abs()\n\n# 3. Set thresholds (adjust as needed)\nrf_thresh = 0.025\ncorr_thresh = 0.05\n\n# 4. Filter features above both thresholds\nimportant_rf = rf_importances[rf_importances > rf_thresh].index\nimportant_corr = correlations[correlations > corr_thresh].index\n\n# 5. Take intersection\nselected_features = list(set(important_rf).intersection(set(important_corr)))\n\n# 6. Subset your data\nX_selected = X[selected_features]\n\nprint(f\"Selected {len(selected_features)} features:\")\nprint(selected_features)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected 12 features:\n['TMEM80', 'TRPM2', 'PYCR1', 'MELK', 'NRIP3', 'IFFO2', 'HSD17B14', 'H19', 'SVIP', 'HMGB3', 'FAM241B', 'CLDN3']\n```\n:::\n:::\n\n\n``` python\nSelected 13 features:\n['CLDN3', 'H19', 'SVIP', 'TRPM2', 'MELK', 'tumor_status', 'PYCR1', 'HMGB3', 'FAM241B', 'NRIP3', 'age_at_dx', 'HSD17B14', 'TMEM80']\n```\n\nAnd i think they are so much different,and i prefer pick the important features of random forest\n\n### ‚úÖ Finalize Feature Selection from Random Forest\n\nYou can set a threshold (e.g., importance \\> 0.025) or just pick the top N features. Based on your earlier output, here's how to do it programmatically:\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# Set threshold for importance\nrf_thresh = 0.03\n\n# Filter features above threshold\nselected_rf_features = rf_importances[rf_importances > rf_thresh].sort_values(ascending=False).index.tolist()\n\n# Subset your data\nX_rf_selected = X[selected_rf_features]\n\nprint(f\"Selected {len(selected_rf_features)} features from Random Forest:\")\nprint(selected_rf_features)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected 21 features from Random Forest:\n['SLC39A4', 'C4orf48', 'TMEM80', 'CLDN3', 'H19', 'NRIP3', 'C5', 'TESK1', 'C9orf72', 'MELK', 'PYCR1', 'SVIP', 'TTC39A', 'CLTA', 'ATP23', 'FAM241B', 'TP53I3', 'PRAG1', 'HMGB3', 'HSD17B14', 'TRPM2']\n```\n:::\n:::\n\n\n### üß™ Step-by-Step: Discriminant Analysis with Visualization\n\n#### 1. **Prepare Your Data**\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nX_selected = df[['CLDN3', 'H19', 'SVIP', 'TRPM2', 'MELK', 'tumor_status',\n                 'PYCR1', 'HMGB3', 'FAM241B', 'NRIP3', 'age_at_dx',\n                 'HSD17B14', 'TMEM80']]\ny = df['event']\n```\n:::\n\n\n#### 2. **Train LDA Model**\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\nX_lda = lda.fit_transform(X_selected, y)\n```\n:::\n\n\n#### 3. **Visualize Fisher Projection (2D Scatter)**\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\ncolors = ['blue', 'red']\nfor label in [0, 1]:\n    plt.scatter(X_lda[y == label, 0], X_lda[y == label, 1] if X_lda.shape[1] > 1 else [0]*sum(y==label),\n                label=f'Event {label}', alpha=0.7, color=colors[label])\nplt.title(\"LDA Projection (Fisher Criterion)\")\nplt.xlabel(\"LD1\")\nif X_lda.shape[1] > 1:\n    plt.ylabel(\"LD2\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mlsql_files/figure-docx/cell-35-output-1.png){}\n:::\n:::\n\n\n![](plot-31.png)\n\n#### 4. **Visualize 1D Histogram of LD1**\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 4))\nplt.hist(X_lda[y == 0, 0], bins=30, alpha=0.6, label='Alive', color='blue')\nplt.hist(X_lda[y == 1, 0], bins=30, alpha=0.6, label='Dead', color='red')\nplt.title(\"LDA Component 1 Distribution\")\nplt.xlabel(\"LD1\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mlsql_files/figure-docx/cell-36-output-1.png){}\n:::\n:::\n\n\n![](plot-32.png)\n\n#### 5. **Bayesian Classification Report**\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\nlda.fit(X_train, y_train)\ny_pred = lda.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92        52\n           1       0.67      0.36      0.47        11\n\n    accuracy                           0.86        63\n   macro avg       0.77      0.66      0.69        63\nweighted avg       0.84      0.86      0.84        63\n\n```\n:::\n:::\n\n\n``` python\n... print(classification_report(y_test, y_pred))\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92        52\n           1       0.67      0.36      0.47        11\n\n    accuracy                           0.86        63\n   macro avg       0.77      0.66      0.69        63\nweighted avg       0.84      0.86      0.84        63\n```\n\n---\n\n\n### üß™ Step-by-Step: Cross-Validation with Multiple Models\n\n#### 1. **Prepare Your Data**\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nX_selected = df[['tumor_status', 'H19', 'pack_years_smoked', 'age_at_dx', 'C4orf48', 'CLDN3', 'SLC39A4', 'C5', 'ATP23', 'PRAG1', 'TMEM80', 'MELK', 'SVIP', 'C9orf72', 'TESK1', 'HSD17B14']]\ny = df['event']\n```\n:::\n\n\n#### 2. **Import Models and Tools**\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport lightgbm as lgb\n```\n:::\n\n\n#### 3. **Define Models**\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nmodels = {\n    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n\n}\n```\n:::\n\n\n#### 4. **Cross-Validation Setup**\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscorers = {\n    'Accuracy': make_scorer(accuracy_score),\n    'Precision': make_scorer(precision_score),\n    'Recall': make_scorer(recall_score),\n    'F1': make_scorer(f1_score)\n}\n```\n:::\n\n\n#### 5. **Run Cross-Validation**\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\nfor name, model in models.items():\n    print(f\"\\n{name}\")\n    for score_name, scorer in scorers.items():\n        scores = cross_val_score(model, X_selected, y, cv=cv, scoring=scorer)\n        print(f\"{score_name}: {scores.mean():.3f} ¬± {scores.std():.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nXGBoost\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.756 ¬± 0.023\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision: 0.560 ¬± 0.063\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall: 0.365 ¬± 0.073\nF1: 0.438 ¬± 0.061\n```\n:::\n:::\n\n\nthis is shit actaully\n\n",
    "supporting": [
      "mlsql_files"
    ],
    "filters": []
  }
}